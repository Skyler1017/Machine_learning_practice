{
 "metadata": {
  "name": "",
  "signature": "sha256:d33c58625decb81168e716fb3228fe6f75ab99b40371a63f6d004e5797730aaa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.genfromtxt('Advertising.csv', delimiter=\",\")\n",
      "x_data = data[1:,1:4]\n",
      "y_data = data[1:,4]\n",
      "print (len(x_data[0]))\n",
      "print (len(y_data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3\n",
        "200\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_loss(b, k, x_data, y_data):\n",
      "    loss = 0\n",
      "    for i in range(len(x_data)):\n",
      "        z = 0\n",
      "        for j in range(len(k)):\n",
      "            z += k[j]*x_data[i][j]\n",
      "        loss += (y_data[i] - (z+b))**2\n",
      "    return loss/float(len(x_data)) / 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_discent(x_data, y_data, lr, epoch, k, b):\n",
      "    batch = len(x_data)\n",
      "    for i in range(epoch):\n",
      "        dz = 0\n",
      "        dk = [0 for _ in range(len(k))]\n",
      "        db = 0\n",
      "        for j in range(batch):\n",
      "            z = 0\n",
      "            for n in range(len(k)):\n",
      "                z += k[n]*x_data[j][n]\n",
      "            dz = -(y_data[j] - (z+b))\n",
      "            for n in range(len(k)):\n",
      "                dk[n] += x_data[j][n] * dz / float(batch)\n",
      "            db += dz / float(batch)\n",
      "        for n in range(len(k)):\n",
      "            k[n] -= lr[n] * dk[n]\n",
      "        b -= lr[0] * db\n",
      "    return k, b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = 0\n",
      "k = [0 for _ in range(len(x_data[0]))]\n",
      "epoch = 100\n",
      "lr = [0.01 for _ in range(len(x_data[0]))]\n",
      "print (\"Start b = {0}, k = {1}, loss = {2}\".format(b, k, compute_loss(b, k, x_data, y_data)))\n",
      "print (\"Running.......\")\n",
      "k, b = gradient_discent(x_data, y_data, lr, epoch, k, b)\n",
      "print (\"After {3} epochs: b = {0}, k = {1}, loss = {2}\".format(b, k, compute_loss(b, k, x_data, y_data), epoch))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Start b = 0, k = [0, 0, 0], loss = 111.85812500000002\n",
        "Running.......\n",
        "After 100 epochs: b = -2.4322330516522027e+244, k = [-4.7319453721734467e+246, -5.8392349524681025e+245, -7.7401824766409029e+245], loss = inf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:7: RuntimeWarning: overflow encountered in double_scalars\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}